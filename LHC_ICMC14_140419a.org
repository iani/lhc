#+TITLE: Bridging machine and human processing in low-level live coding of music.

* Abstract
:PROPERTIES:
:DATE:     <2014-04-19 Sat 22:36>
:END:

Current advances in digital fabrication are accompanied by efforts to bring about increased facility in the fabrication of digital circuits. In this context, of tangibility can apply to the intimate contact with objects as programmable entities forming part of the human-material loop in the sense of physical computing. In this paper, we explore the possibilities of making music with very simple circuits, using an equally minimal interface for live interaction with the hardware.

Our aim is to find new ways for experiencing the behavior of circuits and for navigating inside the data space of generative algorithms with musical devices, in  by using minimal interfaces, while involving both human and machine in the "perception" of the musical output.  In our experiment we have focused on the lowest level of the machine language ~\cite{Diapoulis:12}, that of manipulating bits in real-time.  Furthermore, we attempt to tighten the loop between human and machine by introducing a machine listening component which processes the output of the human-machine interaction.  This splits the perceptual feedback loop into a human and a machine part, and makes the final output a joint outcome of both.


* LHC and Tangibility in live music-making

Live coding as a practice blurs the limits between composition and performance (Blackwell and Collins 2005).  It is also considered as a new notation form (Magnusson 2011) or a method for developing notations, albeit in its infancy.  Live coding from scratch usually follows a bottom-up approach, building musical elements incrementally from simple to more complex ones.  Yet from the point of view of the computer, the musician uses high-level abstractions for coding (for example: SinOsc, Synth, Pattern, Routine, Scale).  In this paper we present an approach that attempts to start from low-level computational elements, namely bits and their manipulation through simple automata that make up the building blocks of digital circuits.  Writing code at the lowest level of machine language seems hardly a practical method for constructing music, and no more a usual method for constructing programs.  Our experiment is motivated by the question, whether it is possible to create an intimate link between musician/coder and machine which, while still based on digital programming principles, addresses both the human and the machine in an integrative manner.  By that we mean integrating different functions or faculties such as sensation (input), perception (human/machine listening), planning (computation), action and comprehension as closely connected as possible, in order to form a closely knit whole.  This is a prerequisite for forming the closest possible interactive link between human and machine, one that creates a "tangible" contact between the two.

In this experiment we are not applying any design metaphors from computer music, but search for grammatical interfaces that open the way to hacking, as suggested by Stowell and McLean (2013).  Our starting point is not music, but computational processes.  Our question is how the performer can use computational processes as raw material to form into something approaching a musical performance.  Therefore, we developed devices and interfaces that serve to communicate to computer musicians some fundamental notions and processes of computing machinery, such as bits, symbols, binary representation of numbers, serial transimission, decoding and encoding process, and lexical analysis. We do not ask of the performer any previous experience in programming.  Such an apparatus might have educational uses (Collins 2012).

In our design, we sought to obey the general rule of presenting the user with 7 $\pm$ 2 elements at any moment (usually refered as Miller's law).  We use three input buttons, one reset button, a potensiometer and two LED displays which reflect the current state of the machine.   Equipped with this interface, we sought to develop a bottom-up minimal programming language and environment for live music-making.

We presented first results of this low-level approach to live coding in (Diapoulis and Zannos 2012).  The language we used likely belongs to type-0 grammars (Grune 2007: 72) in the Chomsky hierarchy.  Whether or not this language can be followed by humans remains an open question; we would like to thank Nick Collins who set this question during live.code.fest in Karlsruhe, 2012.  The basic design that serves as starting point is a 3-bit minimal interface that drives a counter coupled to a decoder as generator of musical structures.  As a next step, our aim is to develop interfaces that enable us to explore and experience the behavior of these processes as musical processes at the building block level, that is, as musical phrases or sections comprised of groups of single note events~cite{Miranda:01}  Here we present first results of this approach, which add higher-level processing of the initial output by the machine.  This can in turn be used as musical material, thereby enriching the final outcome.

Futhermore, we have developed a new kind of interface which allows more intimate and direct tactile interaction with the digital circuit through flexible switches that require very small and light movement, and can be operated while remaining almost entirely in contact with the device.

Figure 1 presents schematically the overall setup of our expreriment and its main components.

* Experimental Setup

Our experiment is is based on the combination two elementary blocks of digital design: A counter and a decoder. Both are sequential circuits which can be represented by a finite state machine~\cite{csd120}. The counter is a 3-bit counter machine which operates as the modulo 8 function using 2's complement. The decoding machine is a Huffman decoder which operates with variable length code and uses a combinational encoding process to procudes symbol sequences from an alphabet of four symbols with specific weights. The human agent provides a 3-bit parallel input to the counter by means of three buttons and a potentiometer. We have developed two different machines, one with a fixed clock and one with a variable clock rate. The potentiometer controls the counter's positive edge clock. It it is an external module which applies only to the machine with the variable clock rate. The output from the counter machine is read in serial order by the decoding machine. The decoder has a single bit input, and an output alphabet of four symbols (A,B,C,D).

In the original experiment, both machines were developed using prototype circuit boards and TTL technology. The output from the counter and the decoder are sent to an arduino board, which is connected to SuperCollider via USB cable. We use SimpleMessageSystem arduino's library which is controlled from ArduinoSMS class in SuperCollider. SuperCollider is responsible for real-time sound synthesis. We have mapped counter's output, numbers 1 to 7 to the seven diatonic degrees and 0 (zero) to silence (pause). The four symbols produced from the decoder provides us the opportunity for senondary mapping.   The software used for this experiment was packaged as a Quark for SuperCollider, named LHC.

\includegraphics[scale=0.65]{LHC-GUI2}

Following diagram gives an overview of the experiment (Figure N).

#PICTURE OF SYSTEM - schema
# !!!!!!!!!!!! replace FSM with LHC !!!!!!!!!!!!!!!!
# maybe remove Huffman coding from 2nd context
\includegraphics[scale=0.5]{LHC_system}

* Hardware interface
The first device used for operating this machine consisted of of three input buttons and a potentiometer.  Many different ways can be applied to this concept.  For example, we can apply the 3-bit input for spatial applications.  Regarding the 3D binary cube representation, by mapping each bit (LS..MS) to a bit-plane.  Recently, a more sensitive tactile interface with very light switches was constructed as an alternative way to drive the device.

* Processing and output
The input is provided by the human agent in terms of a 3-bit parallel input.  This drives a counter machine which implement the modulo 8 addition function in 2's complement. The counter transmits in serial order the 3-bit output to Huffman decoder machine.  The final output is a stream of symbols whose alphabe consists of the four symbols (A, B, C, D).  Overall, the output produced by our initial device (2012) had three levels:

1. The output of a variable-rate counter programmed by the 3-bit switch interface.
2. The output of sampling the states of the counter at a steady-rate.
3. The decoding of the sampled states by a Huffmann decoder into a stream whose alphabet consisted of the 4 symbols A, B, C and D.

* Observation of Output: Exploring Palindromes

From the three levels of output described above, output level 2 presents some interesting characteristics.  The clock rate of the counter is adjusted by a knob, while the rate at which the state of the clock is sampled by the software system that receives its output is steady.  This creates a downsampling-artefact which results in quasi-palindromic structures shown in Figures (n and m).  We created a class LHCV in order to simulate this process and explore it in greater detail.

Through this simulation, we can confirm in software the emergence of quasi-palindromic structures which was observed in hardware. In this paper we give the formal framework for describing these phenomena.

Such an approach has applications in education but also in design at all levels. It also opens new ways to approach live coding~\cite{Collins:03}. The value of low-level approach has already been noted~\cite{Bovermann:14}. Here we try to take this approach to the limit.

At the core of the LHCV-sampling simulator is the following algorithm - as coded in SuperCollider:

{verbatim}
{
  Latch.ar(
    Stepper.ar(
      Impulse.ar(
        Line.kr(1,99,9)
      )
   ),
   Impulse.ar(8)
  )
}.plot(9)
\end{verbatim}

The above code produces quasi-palindromic structures as demonstrated in the following plot (Figure 3). The X-axis represents the number of samples and Y-axis represents the diatonic degrees from 1 to 7, and 0 (zero) is for pause.

# QUASI-PLOT1
\includegraphics[scale=0.5]{Figure 1.pdf}

The palindromes were a natural first outcome of the mechanism, and illustrated a way in which such an elementary process can be induced to produce structures that are recogniseable at a higher level - a kind of "emergence".  The next question in this respect is to determine the ratios of counter rate and sampling rate at which such palindromes occur. The first argument of the Latch UGen is the input, while the second is the trigger for latching the value. The Stepper operates as the modulo 8 function and its first argument is the trigger. This observation demonstrates that by applying a linear function into the frequency argument of the trigger (Stepper) is an approach for generate quasi-palindromic structures.   A characteristic example is following code excerpt in the form of a SuperCollider "tweet" (see https://ccrma.stanford.edu/wiki/SuperCollider_Tweets):

\begin{verbatim}
play{p=Impulse;SendTrig.ar(Changed.ar(a=Latch.ar(Stepper.ar(p.ar(Line.kr(99,
1,40,1,0,2))),p.ar(8))),0,a)};OSCFunc({|m|(degree:m[3]).play},'/tr')
\end{verbatim}

Following code builds a GUI for trying out various parameter configurations of the counter-sampling algorithm interactively. Footnote: [The code makes use of the Lilt2 Library by Iannis Zannos].

\begin{verbatim}
(
SynthDef(\mod8, { |clk=1 xclk=1.1 input=1|
	var p=LFPulse;
	var signal = Latch.ar(Stepper.ar(p.ar(xclk), step: input).poll, p.ar(clk));
	Out.ar(0, SinOsc.ar(100*signal.poll))
}).synthGui(
	specs: [
		clk: [0.1, 2.0],
		xclk: [1.0, 20.0],
		input: ControlSpec(0, 7, \lin, 1)
]);
)
\end{verbatim}

* In search for further patterns: A mini-language for LHC (mLHC)

The observations about the emergence of patterns made at the first stage of the experiment above led to the question whether the machine could also detect patterns in the signal, using algorithmic ways of processing the output.  Since the patterns of output 2 were recognizeable by humans our "bet" was what kind of patterns the machine could recognize from the symbol stream that is the output of 3.  To analyse the string of symbols we employed the techniques of regular expressions, which are one of the first tools of choice for such tasks.  These expressions define regular languages, that is formal languages that are equivalent to non-deterministic finite automata (NFA) ~\cite{Grune07}.  We thus defined a mini-regular-language for musical live coding.

``mLHC'' is a regular language in Chomsky hierarchy. The alphabet of that language consists of the output symbols from the decoder/encoder. Each word is being recognised at run-time through lexical analysis with POSIX expressions.

** Alphabet

The alphabet of our language consists of three letters (symbols) and the empty string {\varepsilon}. Symbol \textit{A} is mapped to \varepsilon (A \rightarrow \varepsilon). In such a way we can reduce the complexity of the tokens. So the alphabet is \Sigma = { \Beta, C, D }.

** Language

We define the language L which contains every product of the alphabet \Sigma^{*} which ends with the letter D, as follows:

L = { w \epsilon \Sigma^{*} : w every word that ends with a D }

** Regular expressions

Using the following POSIX expressions we can recognize every token which ends with a 'D', which is used as an end-marker. The set of the accepted words have an infinite cardinality, though they can be expressed by a finite state automaton (Grune 2007).

\begin{verbatim}
// POSIX expression
D | B+D | C+D | (B+C+)+D | (C+B+)+D | (B+C+)+B+D | (C+B+)+C+D
\end{verbatim}

\noindent Where plus (+) symbol, stands for ``at least one''.

** Graph for lexical analysis
The following picture shows the non determistic automaton which
describes visually the recognisition process on the ongoing output
string from the encoder.
#+COMMENT the D-state DOES NOT have a D-transition!!!!
\includegraphics[scale=0.7]{NFA-mLHC.png}

The start state is S and the accept state is D; \varepsilon - transitions have marked with the latin letter ``e''.

* Discussion: Physiology, Perception, Interaction

The crucial question underlying these experiments concerns the relationship of unconscious and consious processes in musical experience.  Is it possible to conduct music making through programming in a similar way as traditional live music making activities, that is, to involve the intuitive (unconscious) and physical levels of the creative process together with the highly analytical processes of programming?  Already our interface has been pushing in this direction, since it is possible to run the clock rate at the limits of the perception of individual notes.

** Fast vs. Slow
It is inevitable that next generations will be faster in their interaction with the machines. We could imagine future systems of HCI that will improve our capabilities into this (video Collins speed coding).   Speed matters in evolution (Hikosaka 2013) but this is not the case in art practices.  Slow coding represent a completely different perspective into this. But we are making music. Music is a complex phenomenon and a really demanding task. ``Should music interaction be easy?'' (McDermontetal2013).

* Conclusion

The development of our experiment using software, provides us with some experimental tools for analysis and visual and symbolic representations.  The level of abstraction that we introduce provides a new kind of experience in live coding, and sets new open-questions to the field. Whether or not live coding is just a state of mind (Magnusson 2014) or a self-referential (Collins 2011) phenomenon is something that we might have to elaborate more. But we think that already live coders have been doing well as they have already introduced new aspects in technological advents, that of transparent procedures (show us your screens). Usually technology is used to withhold user's faults, where this is not the case in a live coding performance.

A further question concerns the role perceptual and cognitive processing limits in live coding and human-machine interaction.  We used as a typical time frame 0.5 seconds (tempo = 120bpm), and by accellerating beyond that, limits of music perception (Koelsch and Siebel 2005) could be felt.  Experimenting using such an apparatus for live music-making relies on subconscious processes.  Whether or not this can be used as an expressive way to live coding is a question still open to further research.  But we believe that by ``designing constrains'' (Magnusson 2010) using grammatical interfaces for musical expression is a promising field for experimentations as it is a new area of musical practices based on computation.  In our setup we could also observe the computational algorithms (symbolic) juxtaposed to the simulation of signal processes that belong to the domain of Calculus.

Into this scope ``constraints are seen as compositional rules'' (Magnusson). Whether or not this is for real-time or non real-time usage is a matter of the composer/performer. Interesting applications could be developed also on the microscopic level.  This may lead to states of training where programming will become as effortless as swimming in a ``pool of code'' (soup alhabet - Hofstadter:658).  Perhaps this is at least one part of the essence of live coding and interactive programming.  The deduction of the cognitive effort plus a journey in minimal expressions.  A typical duration for a live coding performace is ten to twenty minutes.  Code expressions must be elegant and short, in order to be coherent and easy to debug.  Whether or not it is feasible to write programs unconsciously is a subject for research that is a subject both for human-computer interaction and for philosophy.

* References
- Bovermann, T. and D. Griffiths (2014). ``Computation as material in
  live coding''. MIT Press
- Collins, N., A. McLean, J. Rorhruber and A. Ward. (2003). ``Live
  coding in laptop performance''. Organised Sound 8(3):
  321-330. Cambridge University Press.
- Collins, N. (2011). ``Live coding of consequence''.
- Collins, N. (2012). ``Trading Faures: Virtual Musicians and Machine Ethics''.
- Diapoulis, G. and I. Zannos (2012). ``A minimal interface for live
  hardware coding''. In Live Interfaces 2012, ICSRiM, Leeds University.
- Grune, D. (2007). Parsing techniques: A practical guide.
- Hikosaka etal (2013). ``Why skill matters''.
- Hofstander, D. (1985). ``Questing for the essence of mind and pattern''.
- Lerdahl (1983), §6.2, pg.128 (time-span tree and metrical
  structures)
  §9.2, pg.213 - Prolongation reduction well-formedness rules
  - see pg. 214 - 4 rules (4. no crossing branching)
- Koelsch, Siebel. (2005). ``Towards a neural basis of music
  perception''. Trends in cognitive science.
- Magnusson (2010). ``Designing Constraints''. MIT Press
- Magnusson. T. (2011). ``Algorithms as Scores: Coding Live
  Music''. Leonardo Music Journal, Vol: 21, pp 19-23, 2011. MIT Press.
- Magnusson (2014). ``Herding Cats: Observing Live Coding in the
  wild''. MIT Press
- Miranda (2001). ``Composing music with computers''.
- Patel (2003). ``Language, music, syntax and the brain''. Review
  Nature neuroscience.
- Snyder, B. (2000). ``Music and Memory''. MIT Press
- Stowell, D., and A. McLean (2010). ``Live music-making: a rich open
  task requires a rich open interface''
- James McDermott, Toby Gifford, Anders Bouwer, and Mark Wagy (2013). ``Should Music Interaction Be Easy?''
